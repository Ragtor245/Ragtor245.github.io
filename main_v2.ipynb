{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f88e92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ef7b6f",
   "metadata": {},
   "source": [
    "# Install All Required Libraries\n",
    "\n",
    "1. Huggingface libraries to use Mistral 7B LLM (Open Source LLM)\n",
    "\n",
    "2. LangChain library to call LLM to generate reposne based on the prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb584eb4",
   "metadata": {},
   "source": [
    "Import required Libraries and check whether we have access to GPU or not.\n",
    "\n",
    "We must be getting following output after running the cell\n",
    "\n",
    "device: cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e95e182a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, BitsAndBytesConfig\n",
    "import torch\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import ConversationalRetrievalChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78af75ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0+cu118\n"
     ]
    }
   ],
   "source": [
    "# python !!!! æ‰¾åˆ°bugäº†\n",
    "# æ˜¯torchçš„ç‰ˆæœ¬ä¸å¯¹\n",
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6d02917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb10b324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device:\", device)\n",
    "if device == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2671da31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a8cc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "# åœ¨ç»ˆç«¯ä¸­è¾“å…¥ huggingface-cli login\n",
    "# huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b53ee028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type='fp4',\n",
    "    bnb_4bit_compute_dtype='float16'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3726496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# åªç”¨ç™»å½•ä¸€æ¬¡\\nfrom huggingface_hub import notebook_login\\nnotebook_login(\"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\")\\'\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# åªç”¨ç™»å½•ä¸€æ¬¡\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login(\"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\")'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a49e59c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "226e6c63505c4e83b188d0241cd5790f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "model_name = \"Qwen/Qwen3-8B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"  # Automatically maps the model to the available devices\n",
    "  # Enable sliding window for long context\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017d58e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61ef50f9f8df4168853716c3d2908bcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: <think>\n",
      "å¥½çš„ï¼Œç”¨æˆ·å‘æ¥â€œä½ å¥½â€ï¼Œæˆ‘éœ€è¦å‹å¥½å›åº”ã€‚é¦–å…ˆï¼Œç¡®è®¤ç”¨æˆ·å¯èƒ½çš„éœ€æ±‚ï¼Œå¯èƒ½æ˜¯æµ‹è¯•æˆ–å¼€å§‹å¯¹è¯ã€‚ä¿æŒè‡ªç„¶ï¼Œç”¨ä¸­æ–‡å›åº”ï¼Œé¿å…æœºæ¢°æ„Ÿã€‚å¯ä»¥åŠ å…¥è¡¨æƒ…ç¬¦å·å¢åŠ äº²åˆ‡æ„Ÿï¼ŒåŒæ—¶è¯¢é—®æ˜¯å¦æœ‰éœ€è¦å¸®åŠ©çš„åœ°æ–¹ï¼Œå¼•å¯¼è¿›ä¸€æ­¥äº¤æµã€‚æ³¨æ„è¯­æ°”è½»æ¾ï¼Œç¬¦åˆæ—¥å¸¸èŠå¤©ä¹ æƒ¯ã€‚\n",
      "</think>\n",
      "content: ä½ å¥½å‘€ï¼ğŸ˜Š ä»Šå¤©è¿‡å¾—æ€ä¹ˆæ ·å‘€ï¼Ÿæœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# è°ƒç”¨å¤§æ¨¡å‹è¿›è¡Œå›ç­” ï¼ˆTestå¤§æ¨¡å‹ï¼‰\n",
    "# å¯ä»¥æˆåŠŸè¾“å‡º\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen3-8B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# prepare the model input\n",
    "prompt = \"ä½ å¥½\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# conduct text completion\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768\n",
    ")\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "\n",
    "# parsing thinking content\n",
    "try:\n",
    "    # rindex finding 151668 (</think>)\n",
    "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "except ValueError:\n",
    "    index = 0\n",
    "\n",
    "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "print(\"thinking content:\", thinking_content)\n",
    "print(\"content:\", content)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c9d14b",
   "metadata": {},
   "source": [
    "\n",
    "# **We are using Qwen/Qwen3-8B LLM (Open source from HuggingFace)**\n",
    "\n",
    "If you're new to Hugging Face or machine learning tasks, the following lines of code introduce some scary concepts. This Mistral 7B LLM, although big in size (approximately 14 GB), presents a challenge when loading on a Colab notebook, requiring larger GPUs and making it incompatible with the free T4 GPU.\n",
    "\n",
    "To address this issue, we employ a sharded version of the Mistral LLM. This version is loaded in smaller increments (approximately 1.9 GB each) in the notebook and subsequently aggregated into a single file. This allows us to seamlessly utilize the LLM on a free Colab GPU (T4 GPU) without incurring any costs for learning purposes.\n",
    "\n",
    "These lines of code incorporate several important concepts, which may initially seem complex but are easily comprehensible:\n",
    "\n",
    "1. BitsAndBytesConfig:\n",
    "\n",
    "  - load_in_4bit\n",
    "  - bnb_4bit_use_double_quant\n",
    "  - bnb_4bit_quant_type\n",
    "  - bnb_4bit_compute_dtype\n",
    "\n",
    "2. AutoModelForCausalLM.from_pretrained\n",
    "\n",
    "3. AutoTokenizer.from_pretrained\n",
    "\n",
    "Despite the use of some technical terminology, don't be discouraged. In the coming days, we'll delve into these concepts in an approachable manner, breaking down the complexities. This includes a detailed understanding of the mentioned configurations, loading procedures, and tokenization processes. Embrace the learning journey; it's simpler than it seems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338ad107",
   "metadata": {},
   "source": [
    "# **Creating pipelines to run LLM at Colab notebook**\n",
    "\n",
    "It uses\n",
    "\n",
    "1. Huggingface transformers pipeline\n",
    "  - This is an object that abstract most of the complex code from the library, offering a simple API dedicated to several tasks, including Named Entity Recognition, Masked Language Modeling, Sentiment Analysis, Feature Extraction and Question Answering.\n",
    "\n",
    "  - We are using \"text-generation\" task. It's sort of chatGPT where you ask question and model respond with answer of that question based on it's intelligence.\n",
    "\n",
    "\n",
    "2. LangChain HuggingFacePipeline\n",
    "  - Integrating \"transformers.pipeline\" with LangChain.\n",
    "  - You may be thinking why we need this. We will be using LangChain extensively later and this is first step to integrate our LLM with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64c51874",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_32320\\2370131816.py:13: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n"
     ]
    }
   ],
   "source": [
    "text_generation_pipeline = transformers.pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=300,\n",
    "    temperature = 0.3,\n",
    "    do_sample=True,\n",
    ")\n",
    "mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37df1633",
   "metadata": {},
   "source": [
    "# **Using CSV File with LangChain CSVLoader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97031a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# csvæ–‡ä»¶å¤„ç† è·³è¿‡\\n\\ninput_path = \\'D:\\\\ClassMaterial\\\\graphrag\\\\data_test\\\\é—¨è¯Šå°±è¯Šä¿¡æ¯.csv\\'\\noutput_path = \\'D:\\\\ClassMaterial\\\\graphrag\\\\data_test\\\\é—¨è¯Šå°±è¯Šä¿¡æ¯_new.csv\\'\\n\\nwith open(input_path, \\'rb\\') as f_in,      open(output_path, \\'w\\', encoding=\\'gb2312\\') as f_out:\\n\\n    for i, line in enumerate(f_in, 1):\\n        try:\\n            # å…ˆè§£ç æˆå­—ç¬¦ä¸²å†å†™å›å­—èŠ‚ï¼ˆè‡ªåŠ¨ç¼–ç ï¼‰\\n            decoded_line = line.decode(\\'gb2312\\')\\n            f_out.write(decoded_line)\\n        except UnicodeDecodeError:\\n            print(f\"ç¬¬ {i} è¡ŒåŒ…å«éæ³•å­—ç¬¦ï¼Œå·²è·³è¿‡\")\\n            continue\\n        except Exception as e:\\n            print(f\"ç¬¬ {i} è¡Œå‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}\")\\n            continue\\n\\nprint(\"å¤„ç†å®Œæˆï¼Œæ— æ•ˆè¡Œå·²è‡ªåŠ¨åˆ é™¤\")\\n\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# csvæ–‡ä»¶å¤„ç† è·³è¿‡\n",
    "\n",
    "input_path = 'D:\\ClassMaterial\\graphrag\\data_test\\é—¨è¯Šå°±è¯Šä¿¡æ¯.csv'\n",
    "output_path = 'D:\\ClassMaterial\\graphrag\\data_test\\é—¨è¯Šå°±è¯Šä¿¡æ¯_new.csv'\n",
    "\n",
    "with open(input_path, 'rb') as f_in, \\\n",
    "     open(output_path, 'w', encoding='gb2312') as f_out:\n",
    "\n",
    "    for i, line in enumerate(f_in, 1):\n",
    "        try:\n",
    "            # å…ˆè§£ç æˆå­—ç¬¦ä¸²å†å†™å›å­—èŠ‚ï¼ˆè‡ªåŠ¨ç¼–ç ï¼‰\n",
    "            decoded_line = line.decode('gb2312')\n",
    "            f_out.write(decoded_line)\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"ç¬¬ {i} è¡ŒåŒ…å«éæ³•å­—ç¬¦ï¼Œå·²è·³è¿‡\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"ç¬¬ {i} è¡Œå‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "print(\"å¤„ç†å®Œæˆï¼Œæ— æ•ˆè¡Œå·²è‡ªåŠ¨åˆ é™¤\")\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "465c2396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# csvæ–‡ä»¶å¤„ç† è·³è¿‡\\n\\ninput_path = \\'D:\\\\ClassMaterial\\\\graphrag\\\\data_test\\\\å…¥é™¢å‡ºé™¢ä¿¡æ¯.csv\\'\\noutput_path = \\'D:\\\\ClassMaterial\\\\graphrag\\\\data_test\\\\å…¥é™¢å‡ºé™¢ä¿¡æ¯_new.csv\\'\\n\\nwith open(input_path, \\'rb\\') as f_in,      open(output_path, \\'w\\', encoding=\\'gb2312\\') as f_out:\\n\\n    for i, line in enumerate(f_in, 1):\\n        try:\\n            # å…ˆè§£ç æˆå­—ç¬¦ä¸²å†å†™å›å­—èŠ‚ï¼ˆè‡ªåŠ¨ç¼–ç ï¼‰\\n            decoded_line = line.decode(\\'gb2312\\')\\n            f_out.write(decoded_line)\\n        except UnicodeDecodeError:\\n            print(f\"ç¬¬ {i} è¡ŒåŒ…å«éæ³•å­—ç¬¦ï¼Œå·²è·³è¿‡\")\\n            continue\\n        except Exception as e:\\n            print(f\"ç¬¬ {i} è¡Œå‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}\")\\n            continue\\n\\nprint(\"å¤„ç†å®Œæˆï¼Œæ— æ•ˆè¡Œå·²è‡ªåŠ¨åˆ é™¤\")\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# csvæ–‡ä»¶å¤„ç† è·³è¿‡\n",
    "\n",
    "input_path = 'D:\\ClassMaterial\\graphrag\\data_test\\å…¥é™¢å‡ºé™¢ä¿¡æ¯.csv'\n",
    "output_path = 'D:\\ClassMaterial\\graphrag\\data_test\\å…¥é™¢å‡ºé™¢ä¿¡æ¯_new.csv'\n",
    "\n",
    "with open(input_path, 'rb') as f_in, \\\n",
    "     open(output_path, 'w', encoding='gb2312') as f_out:\n",
    "\n",
    "    for i, line in enumerate(f_in, 1):\n",
    "        try:\n",
    "            # å…ˆè§£ç æˆå­—ç¬¦ä¸²å†å†™å›å­—èŠ‚ï¼ˆè‡ªåŠ¨ç¼–ç ï¼‰\n",
    "            decoded_line = line.decode('gb2312')\n",
    "            f_out.write(decoded_line)\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"ç¬¬ {i} è¡ŒåŒ…å«éæ³•å­—ç¬¦ï¼Œå·²è·³è¿‡\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"ç¬¬ {i} è¡Œå‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "print(\"å¤„ç†å®Œæˆï¼Œæ— æ•ˆè¡Œå·²è‡ªåŠ¨åˆ é™¤\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "004f0b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "loader = CSVLoader(file_path='D:\\ShuyingMi\\\\ragtor\\æ£€éªŒä¿¡æ¯.csv')\n",
    "data = loader.load()\n",
    "# åˆ†å‰²æ–‡æ¡£\n",
    "text_splitter = CharacterTextSplitter(chunk_size=10000, chunk_overlap=20)\n",
    "chunked_docs = text_splitter.split_documents(data)\n",
    "# ä½¿ç”¨HuggingFaceåµŒå…¥\n",
    "embeddings = HuggingFaceEmbeddings(model_name='moka-ai/m3e-base')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4e10070a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "db = FAISS.from_documents(chunked_docs,embeddings)\n",
    "\n",
    "\n",
    "# Connect query to FAISS index using a retriever\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={'k': 10}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3b5c3790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Conversational Retrieval Chain\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(mistral_llm, retriever,return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "52c57fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  ä¸å­˜åœ¨è¯¥æ‚£è€…å°±è¯Šå¡å·çš„æ£€éªŒä¿¡æ¯ã€‚\n",
      "æ ¹æ®æä¾›çš„æ•°æ®ï¼Œæˆ‘éœ€è¦æ‰¾åˆ°ä¸æ‚£è€…å°±è¯Šå¡å·ç \"D8E0B803CA9AF9033BD5A321977B9016984A353C7D4C5488\"ç›¸å…³çš„æ£€éªŒä¿¡æ¯ã€‚é¦–å…ˆï¼Œæˆ‘ä¼šæ£€æŸ¥æ‰€æœ‰ç»™å‡ºçš„æ•°æ®æ¡ç›®ï¼Œå¹¶å¯»æ‰¾åŒ¹é…çš„å°±è¯Šå¡å·ç ã€‚\n",
      "\n",
      "åœ¨æŸ¥çœ‹æ‰€æœ‰çš„è®°å½•åï¼Œå‘ç°æ²¡æœ‰ä¸€æ¡è®°å½•ä¸­çš„æ‚£è€…å°±è¯Šå¡å·ç ä¸ç»™å®šçš„\"D8E0B803CA9AF9033BD5A321977B9016984A353C7D4C5488\"ç›¸åŒ¹é…ã€‚æ‰€æœ‰åˆ—å‡ºçš„å°±è¯Šå¡å·ç ä¸­ï¼Œæœ€è¿‘çš„ä¸€ä¸ªæ˜¯\"ED403CB9832902A5A14D3485DC1F79633748EED79E58FABB\"ï¼Œè€Œå…¶ä»–çš„éƒ½ä¸ç¬¦åˆæ‰€æä¾›çš„å·ç ã€‚\n",
      "\n",
      "å› æ­¤ï¼ŒåŸºäºç°æœ‰çš„ä¿¡æ¯ï¼Œå¯ä»¥ç¡®å®šæ²¡æœ‰å…³äºè¯¥ç‰¹å®šå°±è¯Šå¡å·ç çš„æ£€éªŒä¿¡æ¯è®°å½•ã€‚\n",
      "æ‰€ä»¥ï¼Œç­”æ¡ˆæ˜¯ï¼šä¸å­˜åœ¨è¯¥æ‚£è€…å°±è¯Šå¡å·çš„æ£€éªŒä¿¡æ¯ã€‚\n",
      "å› æ­¤ï¼Œæœ€ç»ˆç­”æ¡ˆæ˜¯ï¼š\\boxed{ä¸å­˜åœ¨è¯¥æ‚£è€…å°±è¯Šå¡å·çš„æ£€éªŒä¿¡æ¯ã€‚}\n",
      "æ ¹æ®ä¸Šè¿°åˆ†æï¼Œæ‚£è€…å°±è¯Šå¡å·ç ä¸ºD8E0B803CA9AF9033\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#We will run an infinite loop to ask questions to LLM and retrieve answers untill the user wants to quit\n",
    "import sys\n",
    "chat_history = []\n",
    "query = input('Prompt: ')\n",
    "# Ask question related CSV file like Which country has highest green gas?\n",
    "result = qa_chain.invoke({'question': query, 'chat_history': chat_history})\n",
    "print('Answer: ' + result['answer'] + '\\n')\n",
    "chat_history.append((query, result['answer']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cee7b8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ‚£è€…å°±è¯Šå¡è¯å·ç : 4917F7AC0DB4187ECBA4E2BC50155797ADDA00FFAC730826\n",
      "é—¨è¯Šæ€¥è¯Šä½é™¢ä½“æ£€æ ‡è¯†: 3\n",
      "é—¨è¯Šæ€¥è¯Šä½é™¢ä½“æ£€ç¼–å·: 75A2A97AA275DD2DCC82584EE1CA7860\n",
      "æ£€éªŒç”³è¯·å•ç¼–å·: FE543EC342942A317DA6162DD5C670F9\n",
      "æ ·å“åˆ†ç±»ä»£ç : 0102\n",
      "æ ·å“åˆ†ç±»åç§°: è¡€æ¸…\n",
      "æ ‡æœ¬é‡‡æ ·æ—¶é—´: 2020/6/2\n",
      "é‡‡æ ·å•ä½ç¼–ç : 320113466002673\n",
      "æ£€éªŒæŠ¥å‘Šç¼–å·: FE543EC342942A317DA6162DD5C670F9\n",
      "æ£€éªŒæŠ¥å‘Šåç§°: æ€¥è¯Šè‚¾åŠŸèƒ½(æ€¥è¯Š1)+æ€¥è¯Šè‚åŠŸèƒ½(æ€¥è¯Š1)+æ€¥è¯Šç¦»å­(æ€¥è¯Š1)\n",
      "æ£€éªŒé¡¹ç›®æ ‡å‡†ç¼–ç : 80306000\n",
      "æ£€éªŒç»“æœ: 5.5\n",
      "æ£€éªŒç»“æœè®¡é‡å•ä½: Î¼mol/L\n",
      "æ£€éªŒç»“æœæ˜¯å¦å¼‚å¸¸: 0\n",
      "æ£€éªŒæ–¹æ³•: \n",
      "æ£€éªŒç§‘å®¤ç¼–ç : A30\n",
      "æ£€éªŒç§‘å®¤åç§°: æ£€éªŒç§‘\n",
      "æ£€éªŒæ—¥æœŸå’Œæ—¶é—´: 2020/6/2\n",
      "\n",
      "---\n",
      "\n",
      "æ‚£è€…å°±è¯Šå¡è¯å·ç : 4917F7AC0DB4187ECBA4E2BC50155797ADDA00FFAC730826\n",
      "é—¨è¯Šæ€¥è¯Šä½é™¢ä½“æ£€æ ‡è¯†: 3\n",
      "é—¨è¯Šæ€¥è¯Šä½é™¢ä½“æ£€ç¼–å·: 75A2A97AA275DD2D779D0F93A38A3857\n",
      "æ£€éªŒç”³è¯·å•ç¼–å·: 318C1EDB512A53208B95E8652FE661B3\n",
      "æ ·å“åˆ†ç±»ä»£ç : 0102\n",
      "æ ·å“åˆ†ç±»åç§°: è¡€æ¸…\n",
      "æ ‡æœ¬é‡‡æ ·æ—¶é—´: 2020/1/20\n",
      "é‡‡æ ·å•ä½ç¼–ç : 320113466002673\n",
      "æ£€éªŒæŠ¥å‘Šç¼–å·: 318C1EDB512A53208B95E8652FE661B3\n",
      "æ£€éªŒæŠ¥å‘Šåç§°: æ€¥è¯Šè‚¾åŠŸèƒ½(æ€¥è¯Š1)+æ€¥è¯Šè‚åŠŸèƒ½(æ€¥è¯Š1)+æ€¥è¯Šç¦»å­(æ€¥è¯Š1)\n",
      "æ£€éªŒé¡¹ç›®æ ‡å‡†ç¼–ç : 80306000\n",
      "æ£€éªŒç»“æœ: 4.5\n",
      "æ£€éªŒç»“æœè®¡é‡å•ä½: Î¼mol/L\n",
      "æ£€éªŒç»“æœæ˜¯å¦å¼‚å¸¸: 0\n",
      "æ£€éªŒæ–¹æ³•: \n",
      "æ£€éªŒç§‘å®¤ç¼–ç : A30\n",
      "æ£€éªŒç§‘å®¤åç§°: æ£€éªŒç§‘\n",
      "æ£€éªŒæ—¥æœŸå’Œæ—¶é—´: 2020/1/20\n",
      "\n",
      "---\n",
      "\n",
      "æ‚£è€…å°±è¯Šå¡è¯å·ç : 8A5CAD8B3C86B4959766C3B2D2382D1ECF97A55365E150ED\n",
      "é—¨è¯Šæ€¥è¯Šä½é™¢ä½“æ£€æ ‡è¯†: 3\n",
      "é—¨è¯Šæ€¥è¯Šä½é™¢ä½“æ£€ç¼–å·: A958C13DD70C4CE59DCF77D872E19C17\n",
      "æ£€éªŒç”³è¯·å•ç¼–å·: 788B7196CC084777D231C6CEC9403A4D\n",
      "æ ·å“åˆ†ç±»ä»£ç : 0102\n",
      "æ ·å“åˆ†ç±»åç§°: è¡€æ¸…\n",
      "æ ‡æœ¬é‡‡æ ·æ—¶é—´: 2021/10/15\n",
      "é‡‡æ ·å•ä½ç¼–ç : 320113466002673\n",
      "æ£€éªŒæŠ¥å‘Šç¼–å·: 788B7196CC084777D231C6CEC9403A4D\n",
      "æ£€éªŒæŠ¥å‘Šåç§°: æ€¥è¯Šè‚¾åŠŸèƒ½(æ€¥è¯Š1)+æ€¥è¯Šè‚åŠŸèƒ½(æ€¥è¯Š1)+æ€¥è¯Šç¦»å­(æ€¥è¯Š1)\n",
      "æ£€éªŒé¡¹ç›®æ ‡å‡†ç¼–ç : 80306000\n",
      "æ£€éªŒç»“æœ: 3.5\n",
      "æ£€éªŒç»“æœè®¡é‡å•ä½: Î¼mol/L\n",
      "æ£€éªŒç»“æœæ˜¯å¦å¼‚å¸¸: 0\n",
      "æ£€éªŒæ–¹æ³•: \n",
      "æ£€éªŒç§‘å®¤ç¼–ç : A30\n",
      "æ£€éªŒç§‘å®¤åç§°: æ£€éªŒç§‘\n",
      "æ£€éªŒæ—¥æœŸå’Œæ—¶é—´: 2021/10/15\n",
      "\n",
      "---\n",
      "\n",
      "æ‚£è€…å°±è¯Šå¡è¯å·ç : 8A5CAD8B3C86B4959766C3B2D2382D1ECF97A55365E150ED\n",
      "é—¨è¯Šæ€¥è¯Šä½é™¢ä½“æ£€æ ‡è¯†: 3\n",
      "é—¨è¯Šæ€¥è¯Šä½é™¢ä½“æ£€ç¼–å·: A958C13DD70C4CE5C19917ACCF5DBCDF\n",
      "æ£€éªŒç”³è¯·å•ç¼–å·: 5F6DDA65E613F4F9A8AEA51CA9F904DA\n",
      "æ ·å“åˆ†ç±»ä»£ç : 0102\n",
      "æ ·å“åˆ†ç±»åç§°: è¡€æ¸…\n",
      "æ ‡æœ¬é‡‡æ ·æ—¶é—´: 2021/7/24\n",
      "é‡‡æ ·å•ä½ç¼–ç : 320113466002673\n",
      "æ£€éªŒæŠ¥å‘Šç¼–å·: 5F6DDA65E613F4F9A8AEA51CA9F904DA\n",
      "æ£€éªŒæŠ¥å‘Šåç§°: æ€¥è¯Šè‚åŠŸèƒ½(æ€¥è¯Š1)+æ€¥è¯Šè‚¾åŠŸèƒ½(æ€¥è¯Š1)+æ€¥è¯Šç¦»å­(æ€¥è¯Š1)\n",
      "æ£€éªŒé¡¹ç›®æ ‡å‡†ç¼–ç : 80306000\n",
      "æ£€éªŒç»“æœ: 3.5\n",
      "æ£€éªŒç»“æœè®¡é‡å•ä½: Î¼mol/L\n",
      "æ£€éªŒç»“æœæ˜¯å¦å¼‚å¸¸: 0\n",
      "æ£€éªŒæ–¹æ³•: \n",
      "æ£€éªŒç§‘å®¤ç¼–ç : A30\n",
      "æ£€éªŒç§‘å®¤åç§°: æ£€éªŒç§‘\n",
      "æ£€éªŒæ—¥æœŸå’Œæ—¶é—´: 2021/7/24\n",
      "\n",
      "---\n",
      "\n",
      "æ‚£è€…å°±è¯Šå¡è¯å·ç : 8A5CAD8B3C86B4953FECEAE038C728C99497C98CC98123FC\n",
      "é—¨è¯Šæ€¥è¯Šä½é™¢ä½“æ£€æ ‡è¯†: 3\n",
      "é—¨è¯Šæ€¥è¯Šä½é™¢ä½“æ£€ç¼–å·: 1F9BCB7F6924CAC52C3342D0ED822F77\n",
      "æ£€éªŒç”³è¯·å•ç¼–å·: 89488BECF99E61F2597663EAEE227018\n",
      "æ ·å“åˆ†ç±»ä»£ç : 0102\n",
      "æ ·å“åˆ†ç±»åç§°: è¡€æ¸…\n",
      "æ ‡æœ¬é‡‡æ ·æ—¶é—´: 2020/9/29\n",
      "é‡‡æ ·å•ä½ç¼–ç : 320113466002673\n",
      "æ£€éªŒæŠ¥å‘Šç¼–å·: 89488BECF99E61F2597663EAEE227018\n",
      "æ£€éªŒæŠ¥å‘Šåç§°: æ€¥è¯Šç¦»å­(æ€¥è¯Š1)+æ€¥è¯Šè‚¾åŠŸèƒ½(æ€¥è¯Š1)+æ€¥è¯Šè‚åŠŸèƒ½(æ€¥è¯Š1)\n",
      "æ£€éªŒé¡¹ç›®æ ‡å‡†ç¼–ç : 80306000\n",
      "æ£€éªŒç»“æœ: 9.6\n",
      "æ£€éªŒç»“æœè®¡é‡å•ä½: Î¼mol/L\n",
      "æ£€éªŒç»“æœæ˜¯å¦å¼‚å¸¸: 0\n",
      "æ£€éªŒæ–¹æ³•: \n",
      "æ£€éªŒç§‘å®¤ç¼–ç : A30\n",
      "æ£€éªŒç§‘å®¤åç§°: æ£€éªŒç§‘\n",
      "æ£€éªŒæ—¥æœŸå’Œæ—¶é—´: 2020/9/29\n",
      "\n",
      "---\n",
      "\n",
      "æ‚£è€…å°±è¯Šå¡è¯å·ç : 8A5CAD8B3C86B4959766C3B2D2382D1ECF97A55365E150ED\n",
      "é—¨è¯Šæ€¥è¯Šä½é™¢ä½“æ£€æ ‡è¯†: 3\n",
      "é—¨è¯Šæ€¥è¯Šä½é™¢ä½“æ£€ç¼–å·: A958C13DD70C4CE599516AC024409931\n",
      "æ£€éªŒç”³è¯·å•ç¼–å·: 2C7D6B74CCB0C4E75778D3A67AAEC2FF\n",
      "æ ·å“åˆ†ç±»ä»£ç : 0102\n",
      "æ ·å“åˆ†ç±»åç§°: è¡€æ¸…\n",
      "æ ‡æœ¬é‡‡æ ·æ—¶é—´: 2021/9/14\n",
      "é‡‡æ ·å•ä½ç¼–ç : 320113466002673\n",
      "æ£€éªŒæŠ¥å‘Šç¼–å·: 2C7D6B74CCB0C4E75778D3A67AAEC2FF\n",
      "æ£€éªŒæŠ¥å‘Šåç§°: ç”µè§£è´¨ä¸‰é¡¹(ç”ŸåŒ–1)+è‚¾åŠŸèƒ½å…¨å¥—ï¼ˆç”ŸåŒ–1ï¼‰+è‚åŠŸèƒ½å…¨å¥—ï¼ˆç”ŸåŒ–1ï¼‰\n",
      "æ£€éªŒé¡¹ç›®æ ‡å‡†ç¼–ç : 80306000\n",
      "æ£€éªŒç»“æœ: 2.2\n",
      "æ£€éªŒç»“æœè®¡é‡å•ä½: Î¼mol/L\n",
      "æ£€éªŒç»“æœæ˜¯å¦å¼‚å¸¸: 0\n",
      "æ£€éªŒæ–¹æ³•: \n",
      "æ£€éªŒç§‘å®¤ç¼–ç : A30\n",
      "æ£€éªŒç§‘å®¤åç§°: æ£€éªŒç§‘\n",
      "æ£€éªŒæ—¥æœŸå’Œæ—¶é—´: 2021/9/14\n",
      "\n",
      "---\n",
      "\n",
      "æ‚£è€…å°±è¯Šå¡è¯å·ç : 4917F7AC0DB4187ECBA4E2BC50155797ADDA00FFAC730826\n",
      "é—¨è¯Šæ€¥è¯Šä½é™¢ä½“æ£€æ ‡è¯†: 3\n",
      "é—¨è¯Šæ€¥è¯Šä½é™¢ä½“æ£€ç¼–å·: 75A2A97AA275DD2DBC2EDB7868FC07D0\n",
      "æ£€éªŒç”³è¯·å•ç¼–å·: BE5C461903B5D320923C1BDC9354015B\n",
      "æ ·å“åˆ†ç±»ä»£ç : 0102\n",
      "æ ·å“åˆ†ç±»åç§°: è¡€æ¸…\n",
      "æ ‡æœ¬é‡‡æ ·æ—¶é—´: 2021/3/31\n",
      "é‡‡æ ·å•ä½ç¼–ç : 320113466002673\n",
      "æ£€éªŒæŠ¥å‘Šç¼–å·: BE5C461903B5D320923C1BDC9354015B\n",
      "æ£€éªŒæŠ¥å‘Šåç§°: æ€¥è¯Šè‚åŠŸèƒ½(æ€¥è¯Š1)+æ€¥è¯Šè‚¾åŠŸèƒ½(æ€¥è¯Š1)+æ€¥è¯Šç¦»å­(æ€¥è¯Š1)\n",
      "æ£€éªŒé¡¹ç›®æ ‡å‡†ç¼–ç : 80306000\n",
      "æ£€éªŒç»“æœ: 6.4\n",
      "æ£€éªŒç»“æœè®¡é‡å•ä½: Î¼mol/L\n",
      "æ£€éªŒç»“æœæ˜¯å¦å¼‚å¸¸: 0\n",
      "æ£€éªŒæ–¹æ³•: \n",
      "æ£€éªŒç§‘å®¤ç¼–ç : A30\n",
      "æ£€éªŒç§‘å®¤åç§°: æ£€éªŒç§‘\n",
      "æ£€éªŒæ—¥æœŸå’Œæ—¶é—´: 2021/3/31\n",
      "\n",
      "---\n",
      "\n",
      "æ‚£è€…å°±è¯Šå¡è¯å·ç : 8A5CAD8B3C86B4959766C3B2D2382D1ECF97A55365E150ED\n",
      "é—¨è¯Šæ€¥è¯Šä½é™¢ä½“æ£€æ ‡è¯†: 3\n",
      "é—¨è¯Šæ€¥è¯Šä½é™¢ä½“æ£€ç¼–å·: D5F04925E4F02DDD09A2FB49C0651F11\n",
      "æ£€éªŒç”³è¯·å•ç¼–å·: 71753922E79452E8BA56B2A001209B56\n",
      "æ ·å“åˆ†ç±»ä»£ç : 0102\n",
      "æ ·å“åˆ†ç±»åç§°: è¡€æ¸…\n",
      "æ ‡æœ¬é‡‡æ ·æ—¶é—´: 2020/5/3\n",
      "é‡‡æ ·å•ä½ç¼–ç : 320113466002673\n",
      "æ£€éªŒæŠ¥å‘Šç¼–å·: 71753922E79452E8BA56B2A001209B56\n",
      "æ£€éªŒæŠ¥å‘Šåç§°: æ€¥è¯Šç¦»å­(æ€¥è¯Š1)+æ€¥è¯Šè‚åŠŸèƒ½(æ€¥è¯Š1)+æ€¥è¯Šè‚¾åŠŸèƒ½(æ€¥è¯Š1)\n",
      "æ£€éªŒé¡¹ç›®æ ‡å‡†ç¼–ç : 80306000\n",
      "æ£€éªŒç»“æœ: 5.5\n",
      "æ£€éªŒç»“æœè®¡é‡å•ä½: Î¼mol/L\n",
      "æ£€éªŒç»“æœæ˜¯å¦å¼‚å¸¸: 0\n",
      "æ£€éªŒæ–¹æ³•: \n",
      "æ£€éªŒç§‘å®¤ç¼–ç : A30\n",
      "æ£€éªŒç§‘å®¤åç§°: æ£€éªŒç§‘\n",
      "æ£€éªŒæ—¥æœŸå’Œæ—¶é—´: 2020/5/3\n",
      "\n",
      "---\n",
      "\n",
      "æ‚£è€…å°±è¯Šå¡è¯å·ç : 1FBAC9CFCAB43DE6EDEF7F7FC413C270680AC8DFD9FA6D19\n",
      "é—¨è¯Šæ€¥è¯Šä½é™¢ä½“æ£€æ ‡è¯†: 3\n",
      "é—¨è¯Šæ€¥è¯Šä½é™¢ä½“æ£€ç¼–å·: 72CEED03798C99A1408AB74C217F92A0\n",
      "æ£€éªŒç”³è¯·å•ç¼–å·: F78BFBD3718753D677057B88F8A31E88\n",
      "æ ·å“åˆ†ç±»ä»£ç : 0102\n",
      "æ ·å“åˆ†ç±»åç§°: è¡€æ¸…\n",
      "æ ‡æœ¬é‡‡æ ·æ—¶é—´: 2023/3/14 9:43:14\n",
      "é‡‡æ ·å•ä½ç¼–ç : 320113466002673\n",
      "æ£€éªŒæŠ¥å‘Šç¼–å·: F78BFBD3718753D677057B88F8A31E88\n",
      "æ£€éªŒæŠ¥å‘Šåç§°: æ€¥è¯Šç¦»å­(æ€¥è¯Š1)+æ€¥è¯Šè‚åŠŸèƒ½(æ€¥è¯Š1)+æ€¥è¯Šè‚¾åŠŸèƒ½(æ€¥è¯Š1)\n",
      "æ£€éªŒé¡¹ç›®æ ‡å‡†ç¼–ç : 80306000\n",
      "æ£€éªŒç»“æœ: 12.1\n",
      "æ£€éªŒç»“æœè®¡é‡å•ä½: Î¼mol/L\n",
      "æ£€éªŒç»“æœæ˜¯å¦å¼‚å¸¸: 0\n",
      "æ£€éªŒæ–¹æ³•: \n",
      "æ£€éªŒç§‘å®¤ç¼–ç : A30\n",
      "æ£€éªŒç§‘å®¤åç§°: æ£€éªŒç§‘\n",
      "æ£€éªŒæ—¥æœŸå’Œæ—¶é—´: 2023/3/14 10:33:32\n",
      "\n",
      "---\n",
      "\n",
      "æ‚£è€…å°±è¯Šå¡è¯å·ç : 8A5CAD8B3C86B495783D3C3CFC3AFF3BDAFA0B27B07EECC7\n",
      "é—¨è¯Šæ€¥è¯Šä½é™¢ä½“æ£€æ ‡è¯†: 3\n",
      "é—¨è¯Šæ€¥è¯Šä½é™¢ä½“æ£€ç¼–å·: 83BA26D92183B66D4F2FE27A0A532D9A\n",
      "æ£€éªŒç”³è¯·å•ç¼–å·: 4B6E287C928EA88254438292F8ED6CCE\n",
      "æ ·å“åˆ†ç±»ä»£ç : 0102\n",
      "æ ·å“åˆ†ç±»åç§°: è¡€æ¸…\n",
      "æ ‡æœ¬é‡‡æ ·æ—¶é—´: 2022/4/9\n",
      "é‡‡æ ·å•ä½ç¼–ç : 320113466002673\n",
      "æ£€éªŒæŠ¥å‘Šç¼–å·: 4B6E287C928EA88254438292F8ED6CCE\n",
      "æ£€éªŒæŠ¥å‘Šåç§°: æ€¥è¯Šè‚¾åŠŸèƒ½(æ€¥è¯Š1)+æ€¥è¯Šè‚åŠŸèƒ½(æ€¥è¯Š1)+æ€¥è¯Šç¦»å­(æ€¥è¯Š1)\n",
      "æ£€éªŒé¡¹ç›®æ ‡å‡†ç¼–ç : 80306000\n",
      "æ£€éªŒç»“æœ: 3.8\n",
      "æ£€éªŒç»“æœè®¡é‡å•ä½: Î¼mol/L\n",
      "æ£€éªŒç»“æœæ˜¯å¦å¼‚å¸¸: 0\n",
      "æ£€éªŒæ–¹æ³•: \n",
      "æ£€éªŒç§‘å®¤ç¼–ç : A30\n",
      "æ£€éªŒç§‘å®¤åç§°: æ£€éªŒç§‘\n",
      "æ£€éªŒæ—¥æœŸå’Œæ—¶é—´: 2022/4/9\n",
      "\n",
      "---\n",
      "\n",
      "æ‚£è€…å°±è¯Šå¡è¯å·ç : 1FBAC9CFCAB43DE6EDEF7F7FC413C270680AC8DFD9FA6D19\n",
      "é—¨è¯Šæ€¥è¯Šä½é™¢ä½“æ£€æ ‡è¯†: 3\n",
      "é—¨è¯Šæ€¥è¯Šä½é™¢ä½“æ£€ç¼–å·: 93A0A0F56BC25131774979E666F15D76\n",
      "æ£€éªŒç”³è¯·å•ç¼–å·: 32E166E5F726F9533CC39230EA4BC20F\n",
      "æ ·å“åˆ†ç±»ä»£ç : 0102\n",
      "æ ·å“åˆ†ç±»åç§°: è¡€æ¸…\n",
      "æ ‡æœ¬é‡‡æ ·æ—¶é—´: 2021/1/26\n",
      "é‡‡æ ·å•ä½ç¼–ç : 320113466002673\n",
      "æ£€éªŒæŠ¥å‘Šç¼–å·: 32E166E5F726F9533CC39230EA4BC20F\n",
      "æ£€éªŒæŠ¥å‘Šåç§°: è‚¾åŠŸèƒ½å…¨å¥—ï¼ˆç”ŸåŒ–1ï¼‰+è‚åŠŸèƒ½å…¨å¥—ï¼ˆç”ŸåŒ–1ï¼‰\n",
      "æ£€éªŒé¡¹ç›®æ ‡å‡†ç¼–ç : 80306000\n",
      "æ£€éªŒç»“æœ: 4.7\n",
      "æ£€éªŒç»“æœè®¡é‡å•ä½: Î¼mol/L\n",
      "æ£€éªŒç»“æœæ˜¯å¦å¼‚å¸¸: 0\n",
      "æ£€éªŒæ–¹æ³•: \n",
      "æ£€éªŒç§‘å®¤ç¼–ç : A30\n",
      "æ£€éªŒç§‘å®¤åç§°: æ£€éªŒç§‘\n",
      "æ£€éªŒæ—¥æœŸå’Œæ—¶é—´: 2021/1/26\n",
      "\n",
      "---\n",
      "\n",
      "æ‚£è€…å°±è¯Šå¡è¯å·ç : 1FBAC9CFCAB43DE6EDEF7F7FC413C270680AC8DFD9FA6D19\n",
      "é—¨è¯Šæ€¥è¯Šä½é™¢ä½“æ£€æ ‡è¯†: 3\n",
      "é—¨è¯Šæ€¥è¯Šä½é™¢ä½“æ£€ç¼–å·: 1242BBAB48E7BD0AAB22653155966757\n",
      "æ£€éªŒç”³è¯·å•ç¼–å·: 0CA2F55EF08D9D204A27D43DC693CD1F\n",
      "æ ·å“åˆ†ç±»ä»£ç : 0102\n",
      "æ ·å“åˆ†ç±»åç§°: è¡€æ¸…\n",
      "æ ‡æœ¬é‡‡æ ·æ—¶é—´: 2021/9/29\n",
      "é‡‡æ ·å•ä½ç¼–ç : 320113466002673\n",
      "æ£€éªŒæŠ¥å‘Šç¼–å·: 0CA2F55EF08D9D204A27D43DC693CD1F\n",
      "æ£€éªŒæŠ¥å‘Šåç§°: æ€¥è¯Šè‚¾åŠŸèƒ½(æ€¥è¯Š1)+æ€¥è¯Šç¦»å­(æ€¥è¯Š1)+æ€¥è¯Šè‚åŠŸèƒ½(æ€¥è¯Š1)\n",
      "æ£€éªŒé¡¹ç›®æ ‡å‡†ç¼–ç : 80306000\n",
      "æ£€éªŒç»“æœ: 9.0\n",
      "æ£€éªŒç»“æœè®¡é‡å•ä½: Î¼mol/L\n",
      "æ£€éªŒç»“æœæ˜¯å¦å¼‚å¸¸: 0\n",
      "æ£€éªŒæ–¹æ³•: \n",
      "æ£€éªŒç§‘å®¤ç¼–ç : A30\n",
      "æ£€éªŒç§‘å®¤åç§°: æ£€éªŒç§‘\n",
      "æ£€éªŒæ—¥æœŸå’Œæ—¶é—´: 2021/9/29\n",
      "\n",
      "---\n",
      "\n",
      "æ‚£è€…å°±è¯Šå¡è¯å·ç : 8E5EEFC53AE477846BB0914BE80B5AB8B962601FD1BE2109\n",
      "é—¨è¯Šæ€¥è¯Šä½é™¢ä½“æ£€æ ‡è¯†: 3\n",
      "é—¨è¯Šæ€¥è¯Šä½é™¢ä½“æ£€ç¼–å·: 25DA30CEBFCAB6F77E95CF456A09465A\n",
      "æ£€éªŒç”³è¯·å•ç¼–å·: 2C667AFD36A76F43EA2AEB657118D221\n",
      "æ ·å“åˆ†ç±»ä»£ç : 0102\n",
      "æ ·å“åˆ†ç±»åç§°: è¡€æ¸…\n",
      "æ ‡æœ¬é‡‡æ ·æ—¶é—´: 2020/9/10\n",
      "é‡‡æ ·å•ä½ç¼–ç : 320113466002673\n",
      "æ£€éªŒæŠ¥å‘Šç¼–å·: 2C667AFD36A76F43EA2AEB657118D221\n",
      "æ£€éªŒæŠ¥å‘Šåç§°: æ€§æ¿€ç´ å…¨å¥—ï¼ˆå…ç–«Aï¼‰+é™é’™ç´ æµ‹å®šï¼ˆåŒ–å­¦å‘å…‰æ³•ã€è§å…‰å…ç–«+ç”·æ€§è‚¿æ ‡å…¨å¥—ï¼ˆå…ç–«Aï¼‰+\n",
      "æ£€éªŒé¡¹ç›®æ ‡å‡†ç¼–ç : 80403000\n",
      "æ£€éªŒç»“æœ: 1.690\n",
      "æ£€éªŒç»“æœè®¡é‡å•ä½: IU/ml\n",
      "æ£€éªŒç»“æœæ˜¯å¦å¼‚å¸¸: 0\n",
      "æ£€éªŒæ–¹æ³•: \n",
      "æ£€éªŒç§‘å®¤ç¼–ç : A30\n",
      "æ£€éªŒç§‘å®¤åç§°: æ£€éªŒç§‘\n",
      "æ£€éªŒæ—¥æœŸå’Œæ—¶é—´: 2020/9/10\n",
      "\n",
      "---\n",
      "\n",
      "æ‚£è€…å°±è¯Šå¡è¯å·ç : 8A5CAD8B3C86B4959766C3B2D2382D1ECF97A55365E150ED\n",
      "é—¨è¯Šæ€¥è¯Šä½é™¢ä½“æ£€æ ‡è¯†: 3\n",
      "é—¨è¯Šæ€¥è¯Šä½é™¢ä½“æ£€ç¼–å·: A958C13DD70C4CE54B5B8D77DAD3A2CD\n",
      "æ£€éªŒç”³è¯·å•ç¼–å·: 2D40974406038C53B596729722E135B7\n",
      "æ ·å“åˆ†ç±»ä»£ç : 0102\n",
      "æ ·å“åˆ†ç±»åç§°: è¡€æ¸…\n",
      "æ ‡æœ¬é‡‡æ ·æ—¶é—´: 2021/8/17\n",
      "é‡‡æ ·å•ä½ç¼–ç : 320113466002673\n",
      "æ£€éªŒæŠ¥å‘Šç¼–å·: 2D40974406038C53B596729722E135B7\n",
      "æ£€éªŒæŠ¥å‘Šåç§°: ç”µè§£è´¨ä¸‰é¡¹(ç”ŸåŒ–1)+è‚åŠŸèƒ½å…¨å¥—ï¼ˆç”ŸåŒ–1ï¼‰+è‚¾åŠŸèƒ½å…¨å¥—ï¼ˆç”ŸåŒ–1ï¼‰\n",
      "æ£€éªŒé¡¹ç›®æ ‡å‡†ç¼–ç : 80305000\n",
      "æ£€éªŒç»“æœ: 1.25\n",
      "æ£€éªŒç»“æœè®¡é‡å•ä½: \n",
      "æ£€éªŒç»“æœæ˜¯å¦å¼‚å¸¸: 0\n",
      "æ£€éªŒæ–¹æ³•: \n",
      "æ£€éªŒç§‘å®¤ç¼–ç : A30\n",
      "æ£€éªŒç§‘å®¤åç§°: æ£€éªŒç§‘\n",
      "æ£€éªŒæ—¥æœŸå’Œæ—¶é—´: 2021/8/17\n",
      "\n",
      "---\n",
      "\n",
      "æ‚£è€…å°±è¯Šå¡è¯å·ç : D8E0B803CA9AF903BB919CA553AAE6CBC62B4C13898825C4\n",
      "é—¨è¯Šæ€¥è¯Šä½é™¢ä½“æ£€æ ‡è¯†: 3\n",
      "é—¨è¯Šæ€¥è¯Šä½é™¢ä½“æ£€ç¼–å·: A29FBFFF25EC47B4544975EC86728F7F\n",
      "æ£€éªŒç”³è¯·å•ç¼–å·: FDDA20C5425BFC75CBECB59AAB1DCB15\n",
      "æ ·å“åˆ†ç±»ä»£ç : 0102\n",
      "æ ·å“åˆ†ç±»åç§°: è¡€æ¸…\n",
      "æ ‡æœ¬é‡‡æ ·æ—¶é—´: 2022/4/29\n",
      "é‡‡æ ·å•ä½ç¼–ç : 320113466002673\n",
      "æ£€éªŒæŠ¥å‘Šç¼–å·: FDDA20C5425BFC75CBECB59AAB1DCB15\n",
      "æ£€éªŒæŠ¥å‘Šåç§°: æ€¥è¯Šè‚¾åŠŸèƒ½(æ€¥è¯Š1)+æ€¥è¯Šç¦»å­(æ€¥è¯Š1)+æ€¥è¯Šè‚åŠŸèƒ½(æ€¥è¯Š1)\n",
      "æ£€éªŒé¡¹ç›®æ ‡å‡†ç¼–ç : 80306000\n",
      "æ£€éªŒç»“æœ: 16.2\n",
      "æ£€éªŒç»“æœè®¡é‡å•ä½: Î¼mol/L\n",
      "æ£€éªŒç»“æœæ˜¯å¦å¼‚å¸¸: 0\n",
      "æ£€éªŒæ–¹æ³•: \n",
      "æ£€éªŒç§‘å®¤ç¼–ç : A30\n",
      "æ£€éªŒç§‘å®¤åç§°: æ£€éªŒç§‘\n",
      "æ£€éªŒæ—¥æœŸå’Œæ—¶é—´: 2022/4/29\n",
      "\n",
      "---\n",
      "\n",
      "æ‚£è€…å°±è¯Šå¡è¯å·ç : 1FBAC9CFCAB43DE6EDEF7F7FC413C270680AC8DFD9FA6D19\n",
      "é—¨è¯Šæ€¥è¯Šä½é™¢ä½“æ£€æ ‡è¯†: 3\n",
      "é—¨è¯Šæ€¥è¯Šä½é™¢ä½“æ£€ç¼–å·: F9CFFA5D22C481504F274F2FF2F5AFFB\n",
      "æ£€éªŒç”³è¯·å•ç¼–å·: F43F70D95E0A10BC29BD6DB95EC1EBBD\n",
      "æ ·å“åˆ†ç±»ä»£ç : 0102\n",
      "æ ·å“åˆ†ç±»åç§°: è¡€æ¸…\n",
      "æ ‡æœ¬é‡‡æ ·æ—¶é—´: 2021/8/10\n",
      "é‡‡æ ·å•ä½ç¼–ç : 320113466002673\n",
      "æ£€éªŒæŠ¥å‘Šç¼–å·: F43F70D95E0A10BC29BD6DB95EC1EBBD\n",
      "æ£€éªŒæŠ¥å‘Šåç§°: è‚¾åŠŸèƒ½å…¨å¥—ï¼ˆç”ŸåŒ–1ï¼‰+è‚åŠŸèƒ½å…¨å¥—ï¼ˆç”ŸåŒ–1ï¼‰+ç”µè§£è´¨ä¸‰é¡¹(ç”ŸåŒ–1)\n",
      "æ£€éªŒé¡¹ç›®æ ‡å‡†ç¼–ç : 80305000\n",
      "æ£€éªŒç»“æœ: 0.74\n",
      "æ£€éªŒç»“æœè®¡é‡å•ä½: \n",
      "æ£€éªŒç»“æœæ˜¯å¦å¼‚å¸¸: 1\n",
      "æ£€éªŒæ–¹æ³•: \n",
      "æ£€éªŒç§‘å®¤ç¼–ç : A30\n",
      "æ£€éªŒç§‘å®¤åç§°: æ£€éªŒç§‘\n",
      "æ£€éªŒæ—¥æœŸå’Œæ—¶é—´: 2021/8/10\n",
      "\n",
      "---\n",
      "\n",
      "æ‚£è€…å°±è¯Šå¡è¯å·ç : 1FBAC9CFCAB43DE6EDEF7F7FC413C270680AC8DFD9FA6D19\n",
      "é—¨è¯Šæ€¥è¯Šä½é™¢ä½“æ£€æ ‡è¯†: 3\n",
      "é—¨è¯Šæ€¥è¯Šä½é™¢ä½“æ£€ç¼–å·: 16A109EEF07E625755935D247A902AD9\n",
      "æ£€éªŒç”³è¯·å•ç¼–å·: 52C0A625CFDE808F0EC7D6C804FF64A4\n",
      "æ ·å“åˆ†ç±»ä»£ç : 0102\n",
      "æ ·å“åˆ†ç±»åç§°: è¡€æ¸…\n",
      "æ ‡æœ¬é‡‡æ ·æ—¶é—´: 2021/5/26\n",
      "é‡‡æ ·å•ä½ç¼–ç : 320113466002673\n",
      "æ£€éªŒæŠ¥å‘Šç¼–å·: 52C0A625CFDE808F0EC7D6C804FF64A4\n",
      "æ£€éªŒæŠ¥å‘Šåç§°: è‚åŠŸèƒ½å…¨å¥—ï¼ˆç”ŸåŒ–1ï¼‰+ç”µè§£è´¨ä¸‰é¡¹(ç”ŸåŒ–1)+è‚¾åŠŸèƒ½å…¨å¥—ï¼ˆç”ŸåŒ–1ï¼‰\n",
      "æ£€éªŒé¡¹ç›®æ ‡å‡†ç¼–ç : 80305000\n",
      "æ£€éªŒç»“æœ: 0.68\n",
      "æ£€éªŒç»“æœè®¡é‡å•ä½: \n",
      "æ£€éªŒç»“æœæ˜¯å¦å¼‚å¸¸: 1\n",
      "æ£€éªŒæ–¹æ³•: \n",
      "æ£€éªŒç§‘å®¤ç¼–ç : A30\n",
      "æ£€éªŒç§‘å®¤åç§°: æ£€éªŒç§‘\n",
      "æ£€éªŒæ—¥æœŸå’Œæ—¶é—´: 2021/5/26\n",
      "\n",
      "---\n",
      "\n",
      "æ‚£è€…å°±è¯Šå¡è¯å·ç : EC6690789ACEFDEB0B1A2D3874E6CE8CE7A7C07161C4CE1A\n",
      "é—¨è¯Šæ€¥è¯Šä½é™¢ä½“æ£€æ ‡è¯†: 3\n",
      "é—¨è¯Šæ€¥è¯Šä½é™¢ä½“æ£€ç¼–å·: 3D3A9A563EDFECEA1AD281A58897D092\n",
      "æ£€éªŒç”³è¯·å•ç¼–å·: E03FD7DC507C6B862207BD02B949FE29\n",
      "æ ·å“åˆ†ç±»ä»£ç : 0102\n",
      "æ ·å“åˆ†ç±»åç§°: è¡€æ¸…\n",
      "æ ‡æœ¬é‡‡æ ·æ—¶é—´: 2021/3/16\n",
      "é‡‡æ ·å•ä½ç¼–ç : 320113466002673\n",
      "æ£€éªŒæŠ¥å‘Šç¼–å·: E03FD7DC507C6B862207BD02B949FE29\n",
      "æ£€éªŒæŠ¥å‘Šåç§°: æ€§æ¿€ç´ å…¨å¥—ï¼ˆå…ç–«Aï¼‰+é™é’™ç´ æµ‹å®šï¼ˆåŒ–å­¦å‘å…‰æ³•ã€è§å…‰å…ç–«æ³•ï¼‰+è‚¿æ ‡å…­é¡¹ï¼ˆå…ç–«Aï¼‰+\n",
      "æ£€éªŒé¡¹ç›®æ ‡å‡†ç¼–ç : 80403000\n",
      "æ£€éªŒç»“æœ: 27.870\n",
      "æ£€éªŒç»“æœè®¡é‡å•ä½: U/ml\n",
      "æ£€éªŒç»“æœæ˜¯å¦å¼‚å¸¸: 1\n",
      "æ£€éªŒæ–¹æ³•: \n",
      "æ£€éªŒç§‘å®¤ç¼–ç : A30\n",
      "æ£€éªŒç§‘å®¤åç§°: æ£€éªŒç§‘\n",
      "æ£€éªŒæ—¥æœŸå’Œæ—¶é—´: 2021/3/16\n",
      "\n",
      "---\n",
      "\n",
      "æ‚£è€…å°±è¯Šå¡è¯å·ç : 1FBAC9CFCAB43DE6EDEF7F7FC413C270680AC8DFD9FA6D19\n",
      "é—¨è¯Šæ€¥è¯Šä½é™¢ä½“æ£€æ ‡è¯†: 3\n",
      "é—¨è¯Šæ€¥è¯Šä½é™¢ä½“æ£€ç¼–å·: F9CFFA5D22C481504F274F2FF2F5AFFB\n",
      "æ£€éªŒç”³è¯·å•ç¼–å·: F43F70D95E0A10BC29BD6DB95EC1EBBD\n",
      "æ ·å“åˆ†ç±»ä»£ç : 0102\n",
      "æ ·å“åˆ†ç±»åç§°: è¡€æ¸…\n",
      "æ ‡æœ¬é‡‡æ ·æ—¶é—´: 2021/8/10\n",
      "é‡‡æ ·å•ä½ç¼–ç : 320113466002673\n",
      "æ£€éªŒæŠ¥å‘Šç¼–å·: F43F70D95E0A10BC29BD6DB95EC1EBBD\n",
      "æ£€éªŒæŠ¥å‘Šåç§°: è‚¾åŠŸèƒ½å…¨å¥—ï¼ˆç”ŸåŒ–1ï¼‰+è‚åŠŸèƒ½å…¨å¥—ï¼ˆç”ŸåŒ–1ï¼‰+ç”µè§£è´¨ä¸‰é¡¹(ç”ŸåŒ–1)\n",
      "æ£€éªŒé¡¹ç›®æ ‡å‡†ç¼–ç : 80306000\n",
      "æ£€éªŒç»“æœ: 6.5\n",
      "æ£€éªŒç»“æœè®¡é‡å•ä½: Î¼mol/L\n",
      "æ£€éªŒç»“æœæ˜¯å¦å¼‚å¸¸: 0\n",
      "æ£€éªŒæ–¹æ³•: \n",
      "æ£€éªŒç§‘å®¤ç¼–ç : A30\n",
      "æ£€éªŒç§‘å®¤åç§°: æ£€éªŒç§‘\n",
      "æ£€éªŒæ—¥æœŸå’Œæ—¶é—´: 2021/8/10\n",
      "\n",
      "---\n",
      "\n",
      "æ‚£è€…å°±è¯Šå¡è¯å·ç : 1FBAC9CFCAB43DE63BD730D746F52CEF0008C528BBFE2A6F\n",
      "é—¨è¯Šæ€¥è¯Šä½é™¢ä½“æ£€æ ‡è¯†: 3\n",
      "é—¨è¯Šæ€¥è¯Šä½é™¢ä½“æ£€ç¼–å·: 1C7F90AD791E9D104C70A9ADDB60F21D\n",
      "æ£€éªŒç”³è¯·å•ç¼–å·: 6B931C145A392FE3CAE35368A76EF985\n",
      "æ ·å“åˆ†ç±»ä»£ç : 0102\n",
      "æ ·å“åˆ†ç±»åç§°: è¡€æ¸…\n",
      "æ ‡æœ¬é‡‡æ ·æ—¶é—´: 2021/11/9\n",
      "é‡‡æ ·å•ä½ç¼–ç : 320113466002673\n",
      "æ£€éªŒæŠ¥å‘Šç¼–å·: 6B931C145A392FE3CAE35368A76EF985\n",
      "æ£€éªŒæŠ¥å‘Šåç§°: é™é’™ç´ æµ‹å®šï¼ˆåŒ–å­¦å‘å…‰æ³•ã€è§å…‰å…ç–«æ³•ï¼‰+æ€§æ¿€ç´ å…¨å¥—ï¼ˆå…ç–«Aï¼‰+ç”·æ€§è‚¿æ ‡å…¨å¥—ï¼ˆå…ç–«A\n",
      "æ£€éªŒé¡¹ç›®æ ‡å‡†ç¼–ç : 80403000\n",
      "æ£€éªŒç»“æœ: 9.620\n",
      "æ£€éªŒç»“æœè®¡é‡å•ä½: U/ml\n",
      "æ£€éªŒç»“æœæ˜¯å¦å¼‚å¸¸: 0\n",
      "æ£€éªŒæ–¹æ³•: \n",
      "æ£€éªŒç§‘å®¤ç¼–ç : A30\n",
      "æ£€éªŒç§‘å®¤åç§°: æ£€éªŒç§‘\n",
      "æ£€éªŒæ—¥æœŸå’Œæ—¶é—´: 2021/11/9\n"
     ]
    }
   ],
   "source": [
    "results = db.similarity_search_with_score(\"æ ¹æ®æˆ‘ç»™å‡ºçš„csvï¼Œæ‚£è€…ç¼–å·ä¸º1FBAC9CFCAB43DE65F00786206D0B9F41F907E5B4A652D0Bçš„æ‚£è€…çš„æ‰€æœ‰æ£€æŸ¥è®°å½•\", k=20)\n",
    "\n",
    "context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "print(context_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "981dd2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç­”æ¡ˆæ˜¯______ã€‚\n",
      "\n",
      "Assistant: æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæˆ‘ä»¬å¯ä»¥ç¡®å®šè¯¥æ‚£è€…çš„å°±è¯Šå¡å·ç ä¸ºD8E0B803CA9AF9030016738B3CA923096DC7D3DFFFEE861Fã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦æ‰¾åˆ°ä¸è¿™ä¸ªç‰¹å®šå°±è¯Šå¡å·ç å¯¹åº”çš„æ£€æŸ¥æ—¶é—´ã€‚\n",
      "\n",
      "é¦–å…ˆï¼Œæˆ‘ä»¬æŸ¥çœ‹\"æ‚£è€…å°±è¯Šå¡è¯å·ç ï¼š1FBAC9CFCAB43DE660D558BB9570C2DDE6B202753E3F09B7\"è¿™ä¸€è¡Œï¼Œå¯¹åº”çš„æ˜¯2023å¹´2æœˆ9æ—¥8æ—¶55åˆ†ã€‚\n",
      "\n",
      "æ¥ç€ï¼Œæˆ‘ä»¬å†æŸ¥ä¸€ä¸‹å…¶ä»–å°±è¯Šå¡å·ç ï¼šâ€œæ‚£è€…å°±è¯Šå¡è¯å·ç ï¼š4384658E89BA15C7B1A3F37AD36E83156A9DACCD28AD45B5â€ã€â€œæ‚£è€…å°±è¯Šå¡è¯å·ç ï¼š56853A85DA42FDCF027870784C735E8C79AAFDC7B4F76CC7â€ã€â€œæ‚£è€…å°±è¯Šå¡è¯å·ç ï¼š1FBAC9CFCAB43DE6155CDE2F53D9CCFF4AB528961\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableMap\n",
    "\n",
    "\n",
    "# Assume embeddings, chunked_docs, and mistral_llm are already defined\n",
    "\n",
    "# Initialize FAISS vector store\n",
    "\n",
    "\n",
    "# Define the retriever with MultiQueryRetriever\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate five\n",
    "    different versions of the given user question to retrieve relevant documents from\n",
    "    a vector database. By generating multiple perspectives on the user question, your\n",
    "    goal is to help the user overcome some of the limitations of the distance-based\n",
    "    similarity search. Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\"\"\",\n",
    ")\n",
    "\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=db.as_retriever(),\n",
    "    llm=mistral_llm,\n",
    "    prompt=QUERY_PROMPT\n",
    ")\n",
    "\n",
    "# Create the prompt template\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "\n",
    "# Initialize the output parser\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "query_text = \"æ‚£è€…å°±è¯Šå¡å·ç ä¸ºD8E0B803CA9AF9030016738B3CA923096DC7D3DFFFEE861Fçš„æ‚£è€…åœ¨ä»€ä¹ˆæ—¶é—´è¿›è¡Œäº†æ£€æŸ¥\"\n",
    "# Retrieve context using the retriever\n",
    "retrieved_docs = retriever.get_relevant_documents(query_text)\n",
    "retrieved_context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "# Create the Conversational Retrieval Chain\n",
    "chain = (\n",
    "    RunnableMap({\n",
    "        \"context\": lambda x: retrieved_context,  # Use a lambda to provide the context\n",
    "        \"question\": lambda x: query_text  # Use a lambda to provide the question\n",
    "    })\n",
    "    | prompt_template\n",
    "    | mistral_llm\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "# Invoke the chain and get the response\n",
    "response = chain.invoke({\"context\": retrieved_context, \"question\": query_text})\n",
    "\n",
    "# Print the response\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
